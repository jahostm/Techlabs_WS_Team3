{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob \n",
    "import plotly.express as px \n",
    "import plotly.io as pio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find all csv files in folder \"Stock Data\" -\n",
    "csv_files = glob.glob(\"/Users/jan/Techlabs_Project_Data/Stock_Data/*.csv\") \n",
    "# read all csv files -> all files have the same structure\n",
    "df_list = [pd.read_csv(file) for file in csv_files] \n",
    "#combine all csv files to one dataframe\n",
    "stock_data = pd.concat(df_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For POC we only use Data from Year 2008 & 2009 \n",
    "stock_data['Date']=pd.to_datetime(stock_data['Date'], format= 'ISO8601', utc=True)\n",
    "stock_data = stock_data[stock_data[\"Date\"].dt.year<=2009]\n",
    "#If we want to show the influence from Natural Disaster on stocks, we need the difference (in%) from Close to Open \n",
    "stock_data['percentage_difference_open_close'] = ((stock_data['Close']-stock_data['Open'])/stock_data['Open'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data natural disaster\n",
    "earthquakes_data = pd.read_csv('Eartquakes-1990-2023.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For POC we only use Data from Year 2008 & 2009. \n",
    "earthquakes_data['date']=pd.to_datetime(earthquakes_data['date'], format= 'ISO8601')\n",
    "earthquakes_data = earthquakes_data[(earthquakes_data['date'].dt.year>=2008)&(earthquakes_data['date'].dt.year<=2009)]\n",
    "earthquakes_data=earthquakes_data.drop(columns=['time','status','tsunami'])# Drop columns 'time', 'status', 'tsunami'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare date columns to act as primary key \n",
    "stock_data.rename(columns={'Date':'date'},inplace=True) #rename date columns in stock_data\n",
    "# Make sure that both columns are in the same datetime format\n",
    "earthquakes_data[\"date\"] = pd.to_datetime(earthquakes_data[\"date\"]).dt.date \n",
    "stock_data[\"date\"] = pd.to_datetime(stock_data[\"date\"]).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge stock_data and earthquake_data to one dataframe.primary key is the date\n",
    "disaster_expanded = earthquakes_data.merge(stock_data, on = 'date', how= 'outer')\n",
    "disaster_expanded = disaster_expanded.sort_values(by=[\"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime\n",
    "disaster_expanded[\"date\"] = pd.to_datetime(disaster_expanded[\"date\"])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the Data to all Disasters in \"State\" Japan & Region Japan and Japan Ticker N225 or \n",
    "#if there was a natural disaster on a day when there was no trading day\n",
    "\n",
    "disaster_japan = disaster_expanded[\n",
    "    disaster_expanded[\"state\"].str.contains('Japan', na=False) & \n",
    "    (disaster_expanded[\"Ticker\"].eq('^N225') | disaster_expanded[\"Ticker\"].isna())& (disaster_expanded['magnitudo']>= 6.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variabels that we want to use for the Info Points on the map \n",
    "latitude = disaster_japan[\"latitude\"]\n",
    "longitude = disaster_japan[\"longitude\"]\n",
    "magnitude = disaster_japan[\"magnitudo\"]\n",
    "depth = disaster_japan[\"depth\"]\n",
    "place = disaster_japan ['place']\n",
    "date = disaster_japan ['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the map in the browser\n",
    "pio.renderers.default = \"browser\"  \n",
    "#Our Mapbox Access Token \n",
    "px.set_mapbox_access_token(\"pk.eyJ1IjoidGVjaGxhYnMzIiwiYSI6ImNtNzBib2xyczAwZHoycnBiM2hxZ24zcngifQ.AaSHNEfc-cnR4uDdEO4gsw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Map with Markers for every disaster in our filtered Map for Japan & Region Japan \n",
    "fig = px.density_map(\n",
    "    disaster_japan, \n",
    "    lat=\"latitude\", \n",
    "    lon=\"longitude\", \n",
    "    z=\"magnitudo\",  #colour intensity after magnitude\n",
    "    hover_name=\"place\",  #first name in the markers\n",
    "    zoom=4, #level of zoom \n",
    "    radius= 10, # hight of the markers on the map \n",
    "    color_continuous_scale=px.colors.sequential.Plasma_r, # Turn Color Scheme upside down\n",
    "    map_style=\"light\"  # style of the map \n",
    ")\n",
    "\n",
    "# Karte anzeigen\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
