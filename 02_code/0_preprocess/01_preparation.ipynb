{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part prepares the list of earthquakes by removing the earthquakes and columns that we are not going to include in the model, timewise and magnitude-wise. Our stock data start from 2008, and in our preliminary research we noticed that earthquakes below 6 do not really make an effect (We ran models with lower thresholds than 6 and did not see much of a difference in results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../01_data/01_raw/Eartquakes-1990-2023.csv')\n",
    "df['date_orig']=df['date']\n",
    "df[['date', 'time']] = df['date'].str.split(' ', expand=True)\n",
    "\n",
    "#removing all the earthquakes before January 1, 2008\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "cutoff = pd.Timestamp('2008-01-01') \n",
    "df = df[df['date'] > cutoff]\n",
    "\n",
    "#Removing all the earthquakes below 6 richter magnitude.\n",
    "df = df.loc[df['magnitudo'] >= 6]\n",
    "\n",
    "df.rename(columns={'magnitudo': 'magnitude'}, inplace=True)\n",
    "\n",
    "df = df[['date_orig','date', 'time', 'longitude', 'latitude', 'magnitude', 'depth', 'significance', 'tsunami', 'state','data_type']]\n",
    "df.to_csv('../../01_data/02_pre/clean_major_earthquakes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part combines all our stock market files into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for year in range(2008, 2024):\n",
    "    file_name = f\"../../01_data/01_raw/{year}_Global_Markets_Data.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_name)\n",
    "        df['Year'] = year\n",
    "        dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_name}\")\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df = df.replace({r'\\^': '', r'=': '', r'000001.SS':'000001SS'}, regex=True)\n",
    "df['change'] = df['Close'] - df['Open']\n",
    "df.rename(columns={'Date': 'date','Ticker':'ticker','Open':'open'}, inplace=True)\n",
    "df = df[['ticker','date','open','change']]\n",
    "df.to_csv('../../01_data/02_pre/stocks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This splits the stock market file to files for each market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "Tickers = df['ticker'].unique()\n",
    "for ticker in Tickers:\n",
    "    df_alt = df[df['ticker'] == ticker]\n",
    "    # Define the full directory path\n",
    "    directory = \"../../01_data/02_pre/01_index\"\n",
    "    \n",
    "    # Making sure the directory exists and creating the file\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = f\"{directory}/{ticker}.csv\"\n",
    "    df_alt.to_csv(filename, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
