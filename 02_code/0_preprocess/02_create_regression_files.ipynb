{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from timezonefinder import TimezoneFinder\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>change</th>\n",
       "      <th>change_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NYA</td>\n",
       "      <td>2008-08-01</td>\n",
       "      <td>8438.709961</td>\n",
       "      <td>-59.559570</td>\n",
       "      <td>-0.705790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IXIC</td>\n",
       "      <td>2008-08-01</td>\n",
       "      <td>2326.830078</td>\n",
       "      <td>-15.870117</td>\n",
       "      <td>-0.682049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FTSE</td>\n",
       "      <td>2008-08-01</td>\n",
       "      <td>5411.899902</td>\n",
       "      <td>-57.199707</td>\n",
       "      <td>-1.056925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NSEI</td>\n",
       "      <td>2008-08-01</td>\n",
       "      <td>4331.600098</td>\n",
       "      <td>81.949707</td>\n",
       "      <td>1.891904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BSESN</td>\n",
       "      <td>2008-08-01</td>\n",
       "      <td>14064.259766</td>\n",
       "      <td>592.430664</td>\n",
       "      <td>4.212313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date          open      change  change_pct\n",
       "0    NYA  2008-08-01   8438.709961  -59.559570   -0.705790\n",
       "1   IXIC  2008-08-01   2326.830078  -15.870117   -0.682049\n",
       "2   FTSE  2008-08-01   5411.899902  -57.199707   -1.056925\n",
       "3   NSEI  2008-08-01   4331.600098   81.949707    1.891904\n",
       "4  BSESN  2008-08-01  14064.259766  592.430664    4.212313"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log = pd.read_csv('../../01_data/02_preprocess/stocks.csv')\n",
    "#Calculating the daily change, removing what seems to be unnecessary columns\n",
    "df_log['change_pct'] = df_log['change'] *100/ df_log['open']\n",
    "# Compute bounds (mean & std) per Date across all tickers\n",
    "bounds = df_log.groupby('date')['change_pct'].agg(['mean', 'std']).reset_index()\n",
    "df_log = df_log.drop(columns=['Unnamed: 0'])\n",
    "df_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data for ticker NYA to ../../01_data/03_analysis/NYA.csv\n",
      "Saved data for ticker IXIC to ../../01_data/03_analysis/IXIC.csv\n",
      "Saved data for ticker FTSE to ../../01_data/03_analysis/FTSE.csv\n",
      "Saved data for ticker NSEI to ../../01_data/03_analysis/NSEI.csv\n",
      "Saved data for ticker BSESN to ../../01_data/03_analysis/BSESN.csv\n",
      "Saved data for ticker N225 to ../../01_data/03_analysis/N225.csv\n",
      "Saved data for ticker 000001SS to ../../01_data/03_analysis/000001SS.csv\n",
      "Saved data for ticker N100 to ../../01_data/03_analysis/N100.csv\n",
      "Saved data for ticker DJI to ../../01_data/03_analysis/DJI.csv\n",
      "Saved data for ticker GSPC to ../../01_data/03_analysis/GSPC.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output directory exists\n",
    "output_dir = '../../01_data/03_analysis/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read market and earthquake datasets\n",
    "df_markets = pd.read_csv('../../01_data/02_preprocess/markets_info.csv')\n",
    "df_eq = pd.read_csv('../../01_data/02_preprocess/eq_2008-23.csv')  # Earthquakes â‰¥ 6 from Aug 1, 2008 - end of 2009\n",
    "\n",
    "# Convert date columns to datetime\n",
    "df_eq['date'] = pd.to_datetime(df_eq['date'], format='mixed')\n",
    "\n",
    "# Initialize TimezoneFinder\n",
    "tf = TimezoneFinder()\n",
    "\n",
    "\n",
    "# Process each ticker in the markets.csv file\n",
    "for _, market_row in df_markets.iterrows():\n",
    "    \n",
    "    # LOGISTIC REGRESSION\n",
    "    ticker = market_row['ticker']\n",
    "    # Filter data for the current ticker\n",
    "    ticker_data = df_log[df_log['ticker'] == ticker][['date', 'change_pct']]\n",
    "\n",
    "    # Merge ticker data with precomputed bounds\n",
    "    df_alt_ticker = pd.merge(ticker_data, bounds, on='date')\n",
    "\n",
    "    # Vectorized effect classification using np.select\n",
    "    conditions = [\n",
    "    (df_alt_ticker['mean'] - df_alt_ticker['std'] <= df_alt_ticker['change_pct']),\n",
    "    \n",
    "    (df_alt_ticker['mean'] - 2 * df_alt_ticker['std'] <= df_alt_ticker['change_pct']) & \n",
    "    (df_alt_ticker['change_pct'] < df_alt_ticker['mean'] - df_alt_ticker['std']),\n",
    "    \n",
    "    (df_alt_ticker['mean'] - 3 * df_alt_ticker['std'] <= df_alt_ticker['change_pct']) & \n",
    "    (df_alt_ticker['change_pct'] < df_alt_ticker['mean'] - 2 * df_alt_ticker['std'])\n",
    "]\n",
    "\n",
    "\n",
    "    choices = [0, 1, 2]\n",
    "    df_alt_ticker['effect'] = np.select(conditions, choices, default=3)\n",
    "    # Convert 'Date' column in df_alt_ticker to datetime without time\n",
    "    df_alt_ticker['date'] = pd.to_datetime(df_alt_ticker['date'], errors='coerce').dt.date    \n",
    "#END OF LOGISTIC\n",
    "\n",
    "\n",
    "    ticker = market_row['ticker']\n",
    "    \n",
    "    # Convert latitude and longitude to float\n",
    "    latitude = float(market_row['Latitude'])\n",
    "    longitude = float(market_row['Longitude'])\n",
    "\n",
    "    # Convert close time to datetime.time object\n",
    "    close_time = pd.to_datetime(market_row['close'], format='%H:%M', errors='coerce').time()\n",
    "\n",
    "    # Find the time zone based on latitude and longitude\n",
    "    timezone_str = tf.timezone_at(lat=latitude, lng=longitude)\n",
    "    tz = timezone(timezone_str)\n",
    "\n",
    "    # Read the stock market data for the current ticker\n",
    "    df_ticker = pd.read_csv(f'../../01_data/02_preprocess/01_index/{ticker}.csv')\n",
    "    #df_ticker.rename(columns={'Date': 'date'}, inplace=True)  # Rename 'Date' column to 'date'\n",
    "\n",
    "    # Convert df_ticker date column to the respective time zone\n",
    "    df_ticker['date'] = pd.to_datetime(df_ticker['date']).dt.tz_localize(tz)\n",
    "\n",
    "    # Normalize the earthquake data to the close time of the current ticker\n",
    "    if df_eq['date'].dt.tz is None:\n",
    "        # If the column is timezone-naive, localize it to UTC first\n",
    "        df_eq['date_close'] = df_eq['date'].dt.tz_localize('UTC').dt.tz_convert(tz).dt.normalize() + pd.Timedelta(hours=close_time.hour, minutes=close_time.minute)\n",
    "    else:\n",
    "        # If the column is already timezone-aware, convert it directly\n",
    "        df_eq['date_close'] = df_eq['date'].dt.tz_convert(tz).dt.normalize() + pd.Timedelta(hours=close_time.hour, minutes=close_time.minute)\n",
    "\n",
    "    # Define the coordinates for the current market\n",
    "    market_coords = (latitude, longitude)\n",
    "\n",
    "    # Initialize lists for new columns\n",
    "    num_list, sum_list, max_mag, max_sig, min_depth, min_dist_list, sum_tsunami = [], [], [], [], [], [], []\n",
    "\n",
    "    # Process each row in df_ticker\n",
    "    for i, ticker_row in df_ticker.iterrows():\n",
    "        curr_date = ticker_row['date']\n",
    "        prev_date = df_ticker.iloc[i - 1]['date'] if i > 0 else None\n",
    "\n",
    "        # Define time window: after previous day's close and before current day's close\n",
    "        if prev_date is not None:\n",
    "            eq_filtered = df_eq[(df_eq['date'] > prev_date + pd.Timedelta(hours=close_time.hour, minutes=close_time.minute)) & \n",
    "                               (df_eq['date'] <= curr_date + pd.Timedelta(hours=close_time.hour, minutes=close_time.minute))]\n",
    "        else:\n",
    "            eq_filtered = df_eq[df_eq['date'] <= curr_date + pd.Timedelta(hours=close_time.hour, minutes=close_time.minute)]\n",
    "\n",
    "        # Compute required values\n",
    "        num_list.append(len(eq_filtered))\n",
    "        sum_list.append(eq_filtered['magnitudo'].sum() if not eq_filtered.empty else np.nan)\n",
    "        max_mag.append(eq_filtered['magnitudo'].max() if not eq_filtered.empty else np.nan)\n",
    "        max_sig.append(eq_filtered['significance'].max() if not eq_filtered.empty else np.nan)\n",
    "        min_depth.append(eq_filtered['depth'].min() if not eq_filtered.empty else np.nan)\n",
    "        sum_tsunami.append(eq_filtered['tsunami'].sum() if not eq_filtered.empty else np.nan)\n",
    "        \n",
    "        # Compute distances from the market's location\n",
    "        if not eq_filtered.empty:\n",
    "            distances = eq_filtered.apply(lambda row: geodesic((row['latitude'], row['longitude']), market_coords).km, axis=1)\n",
    "            min_dist_list.append(distances.min())\n",
    "        else:\n",
    "            min_dist_list.append(np.nan)      \n",
    "        \n",
    "    # Create the combined df\n",
    "    df = df_ticker.copy()\n",
    "    df['num'] = num_list\n",
    "    df['sum'] = sum_list\n",
    "    df['max_mag'] = max_mag\n",
    "    df['max_sig'] = max_sig\n",
    "    df['min_depth'] = min_depth\n",
    "    df['min_dist'] = min_dist_list\n",
    "    df['tsunami'] = sum_tsunami\n",
    "    # Convert 'date' column in df to datetime and discard the time part\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    \n",
    "    \n",
    "    # Merge the two DataFrames based on the Date and date columns\n",
    "    merged_df = pd.merge(df_alt_ticker, df, left_on='date', right_on='date', how='inner')    \n",
    "    merged_df = merged_df.drop(columns=['mean', 'std'])\n",
    "    # Reorder columns\n",
    "    merged_df = merged_df[['date', 'change', 'change_pct', 'effect', 'num', 'sum', 'max_mag', 'max_sig', 'min_depth', 'min_dist', 'tsunami']]\n",
    "\n",
    "\n",
    "    # Save the dataframe to a CSV file named according to the ticker\n",
    "    output_file = os.path.join(output_dir, f'{ticker}.csv')\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved data for ticker {ticker} to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
