{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from timezonefinder import TimezoneFinder\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = pd.read_csv('../../01_data/02_pre/stocks.csv')\n",
    "\n",
    "#Calculating the daily change\n",
    "df_log['change_pct'] = df_log['change'] *100/ df_log['open']\n",
    "\n",
    "#Calculating bounds (average and standard deviation for our 'effect')\n",
    "bounds = df_log.groupby('date')['change_pct'].agg(['mean', 'std']).reset_index()\n",
    "df_log = df_log.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data for ticker NYA to ../../01_data/03_analysis/NYA.csv\n",
      "Saved data for ticker IXIC to ../../01_data/03_analysis/IXIC.csv\n",
      "Saved data for ticker FTSE to ../../01_data/03_analysis/FTSE.csv\n",
      "Saved data for ticker NSEI to ../../01_data/03_analysis/NSEI.csv\n",
      "Saved data for ticker BSESN to ../../01_data/03_analysis/BSESN.csv\n",
      "Saved data for ticker N225 to ../../01_data/03_analysis/N225.csv\n",
      "Saved data for ticker 000001SS to ../../01_data/03_analysis/000001SS.csv\n",
      "Saved data for ticker N100 to ../../01_data/03_analysis/N100.csv\n",
      "Saved data for ticker DJI to ../../01_data/03_analysis/DJI.csv\n",
      "Saved data for ticker GSPC to ../../01_data/03_analysis/GSPC.csv\n"
     ]
    }
   ],
   "source": [
    "#Ensure the output directory exists\n",
    "output_dir = '../../01_data/03_analysis/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df_markets = pd.read_csv('../../01_data/02_pre/markets_info.csv')\n",
    "df_eq = pd.read_csv('../../01_data/02_pre/clean_major_earthquakes.csv')\n",
    "\n",
    "df_eq['date'] = pd.to_datetime(df_eq['date'], format='mixed')\n",
    "\n",
    "#We use this to find the timezone of the headquarters of each market\n",
    "tf = TimezoneFinder()\n",
    "\n",
    "#This loop creates all the necessary independent variables for our regressions\n",
    "for _, market_row in df_markets.iterrows():\n",
    "    \n",
    "    #Start of the section for calculating the 'effect' column for the multinomial logistic regression\n",
    "    ticker = market_row['ticker']\n",
    "    ticker_data = df_log[df_log['ticker'] == ticker][['date', 'change_pct']]\n",
    "\n",
    "    df_alt_ticker = pd.merge(ticker_data, bounds, on='date')\n",
    "    #The conditions for the 'effect' are set out here\n",
    "    conditions = [\n",
    "    (df_alt_ticker['mean'] - df_alt_ticker['std'] <= df_alt_ticker['change_pct']),\n",
    "    \n",
    "    (df_alt_ticker['mean'] - 2 * df_alt_ticker['std'] <= df_alt_ticker['change_pct']) & \n",
    "    (df_alt_ticker['change_pct'] < df_alt_ticker['mean'] - df_alt_ticker['std']),\n",
    "    \n",
    "    (df_alt_ticker['mean'] - 3 * df_alt_ticker['std'] <= df_alt_ticker['change_pct']) & \n",
    "    (df_alt_ticker['change_pct'] < df_alt_ticker['mean'] - 2 * df_alt_ticker['std'])\n",
    "]\n",
    "    choices = [0, 1, 2]\n",
    "    df_alt_ticker['effect'] = np.select(conditions, choices, default=3)\n",
    "    df_alt_ticker['date'] = pd.to_datetime(df_alt_ticker['date'], errors='coerce').dt.date    \n",
    "    ##End of the section for variables for multinomial logistic regression\n",
    "\n",
    "    \n",
    "    latitude = float(market_row['Latitude'])\n",
    "    longitude = float(market_row['Longitude'])\n",
    "    close_time = pd.to_datetime(market_row['close'], format='%H:%M', errors='coerce').time()\n",
    "\n",
    "    #Finding the timezone\n",
    "    timezone_str = tf.timezone_at(lat=latitude, lng=longitude)\n",
    "    tz = timezone(timezone_str)\n",
    "\n",
    "    df_ticker = pd.read_csv(f'../../01_data/02_pre/01_index/{ticker}.csv')\n",
    "\n",
    "    #Converting the date column to the timezone of the market\n",
    "    df_ticker['date'] = pd.to_datetime(df_ticker['date']).dt.tz_localize(tz)\n",
    "\n",
    "    if df_eq['date'].dt.tz is None:\n",
    "        #If the column is timezone-naive, localize it to UTC first\n",
    "        df_eq['date_close'] = df_eq['date'].dt.tz_localize('UTC').dt.tz_convert(tz).dt.normalize() + pd.Timedelta(hours=close_time.hour, minutes=close_time.minute)\n",
    "    else:\n",
    "        #If the column is already timezone-aware, convert it directly\n",
    "        df_eq['date_close'] = df_eq['date'].dt.tz_convert(tz).dt.normalize() + pd.Timedelta(hours=close_time.hour, minutes=close_time.minute)\n",
    "\n",
    "    market_coords = (latitude, longitude)\n",
    "\n",
    "    #Start of summarizing earthquakes to our independent variables\n",
    "    num_list, sum_list, max_mag, max_sig, min_depth, min_dist_list, sum_tsunami = [], [], [], [], [], [], []\n",
    "\n",
    "    for i, ticker_row in df_ticker.iterrows():\n",
    "        curr_date = ticker_row['date']\n",
    "        prev_date = df_ticker.iloc[i - 1]['date'] if i > 0 else None\n",
    "\n",
    "        #Time window: After last open day's close and before  today's close\n",
    "        if prev_date is not None:\n",
    "            eq_filtered = df_eq[(df_eq['date'] > prev_date + pd.Timedelta(hours=close_time.hour, minutes=close_time.minute)) & \n",
    "                               (df_eq['date'] <= curr_date + pd.Timedelta(hours=close_time.hour, minutes=close_time.minute))]\n",
    "        else:\n",
    "            eq_filtered = df_eq[df_eq['date'] <= curr_date + pd.Timedelta(hours=close_time.hour, minutes=close_time.minute)]\n",
    "\n",
    "        \n",
    "        num_list.append(len(eq_filtered))\n",
    "        sum_list.append(eq_filtered['magnitudo'].sum() if not eq_filtered.empty else np.nan)\n",
    "        max_mag.append(eq_filtered['magnitudo'].max() if not eq_filtered.empty else np.nan)\n",
    "        max_sig.append(eq_filtered['significance'].max() if not eq_filtered.empty else np.nan)\n",
    "        min_depth.append(eq_filtered['depth'].min() if not eq_filtered.empty else np.nan)\n",
    "        sum_tsunami.append(eq_filtered['tsunami'].sum() if not eq_filtered.empty else np.nan)\n",
    "        \n",
    "        #Finding the distances from the market's location\n",
    "        if not eq_filtered.empty:\n",
    "            distances = eq_filtered.apply(lambda row: geodesic((row['latitude'], row['longitude']), market_coords).km, axis=1)\n",
    "            min_dist_list.append(distances.min())\n",
    "        else:\n",
    "            min_dist_list.append(np.nan)      \n",
    "        \n",
    "    \n",
    "    df = df_ticker.copy()\n",
    "    df['num'] = num_list\n",
    "    df['sum'] = sum_list\n",
    "    df['max_mag'] = max_mag\n",
    "    df['max_sig'] = max_sig\n",
    "    df['min_depth'] = min_depth\n",
    "    df['min_dist'] = min_dist_list\n",
    "    df['tsunami'] = sum_tsunami\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    \n",
    "    \n",
    "    #Merging the two DataFrames created in the loop\n",
    "    merged_df = pd.merge(df_alt_ticker, df, left_on='date', right_on='date', how='inner')    \n",
    "    merged_df = merged_df.drop(columns=['mean', 'std'])\n",
    "    #Reordering columns\n",
    "    merged_df = merged_df[['date', 'change', 'change_pct', 'effect', 'num', 'sum', 'max_mag', 'max_sig', 'min_depth', 'min_dist', 'tsunami']]\n",
    "\n",
    "\n",
    "    output_file = os.path.join(output_dir, f'{ticker}.csv')\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved data for ticker {ticker} to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
