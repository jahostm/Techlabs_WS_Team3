{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a ipynb file here to make it easier to run section-by-section and see how it proceeds after each step. You can just combine all of them in a .py file.\n",
    "\n",
    "Feel free to change any coding or the main regression model. Obviously I'm here for any questions you might have.\n",
    "\n",
    "I think it's easy to adapt this to other stock markets. I'm going to add the notepad file of each market's open and close, and location. You can edit different variables here to adapt it to other markets. Beware of coordinates, and timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticker                      date      change  num  sum  max  avg  \\\n",
      "0  ^N225 2008-08-01 00:00:00+09:00 -181.980469    0  NaN  NaN  NaN   \n",
      "1  ^N225 2008-08-04 00:00:00+09:00 -150.100586    0  NaN  NaN  NaN   \n",
      "2  ^N225 2008-08-05 00:00:00+09:00  -42.349609    1  6.3  6.3  6.3   \n",
      "3  ^N225 2008-08-06 00:00:00+09:00  195.459961    1  6.0  6.0  6.0   \n",
      "4  ^N225 2008-08-07 00:00:00+09:00 -133.000000    0  NaN  NaN  NaN   \n",
      "\n",
      "      min_dist     max_dist  \n",
      "0          NaN          NaN  \n",
      "1          NaN          NaN  \n",
      "2  4818.209478  4818.209478  \n",
      "3  3159.157480  3159.157480  \n",
      "4          NaN          NaN  \n"
     ]
    }
   ],
   "source": [
    "#Preparing and shaping the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "df_eq = pd.read_csv('data/test/EQtest.csv') #EQtest is the data set including earthquakes equal or larger than 6 from August 1, 2008 to the end of 2009\n",
    "df_n225 = pd.read_csv('data/test/^N225.csv') #^N225.csv is the daily changes in N225 stock market from August 1, 2008 to the end of 2009 (the first two files)\n",
    "df_n225.rename(columns={'Date': 'date'}, inplace=True) #The original file has it as Date, but changing to date makes it much easier\n",
    "\n",
    "# Convert df_n225 date column to Japan Standard Time Zone (JST)\n",
    "df_n225['date'] = pd.to_datetime(df_n225['date']).dt.tz_localize('Japan')\n",
    "\n",
    "# Convert df_eq date column and normalize times to 15:25 JST\n",
    "df_eq['date'] = pd.to_datetime(df_eq['date'], format='mixed')\n",
    "df_eq['date_1600'] = df_eq['date'].dt.normalize() + pd.Timedelta(hours=15, minutes=25) #Setting the end of trade on N225 market\n",
    "\n",
    "# Define Tokyo coordinates\n",
    "tokyo_coords = (36.6528, 139.8394)\n",
    "\n",
    "# Initialize lists for new columns\n",
    "num_list, sum_list, max_list, avg_list, min_dist_list, max_dist_list = [], [], [], [], [], []\n",
    "\n",
    "# Process each row in df_n225\n",
    "for i, n225_row in df_n225.iterrows():\n",
    "    curr_date = n225_row['date']\n",
    "    prev_date = df_n225.iloc[i - 1]['date'] if i > 0 else None\n",
    "\n",
    "    # Define time window: after previous day's 15:25 and before current day's 15:25, to make a summary of earthquakes between consecutive end of trade points\n",
    "    if prev_date is not None:\n",
    "        eq_filtered = df_eq[(df_eq['date'] > prev_date + pd.Timedelta(hours=15, minutes=25)) & \n",
    "                             (df_eq['date'] <= curr_date + pd.Timedelta(hours=15, minutes=25))]\n",
    "    else:\n",
    "        eq_filtered = df_eq[df_eq['date'] <= curr_date + pd.Timedelta(hours=15, minutes=25)]\n",
    "\n",
    "    # Compute required values\n",
    "    num_list.append(len(eq_filtered))\n",
    "    sum_list.append(eq_filtered['magnitudo'].sum() if not eq_filtered.empty else np.nan)\n",
    "    max_list.append(eq_filtered['magnitudo'].max() if not eq_filtered.empty else np.nan)\n",
    "    avg_list.append(eq_filtered['magnitudo'].mean() if not eq_filtered.empty else np.nan)\n",
    "\n",
    "    # Compute distances from London\n",
    "    if not eq_filtered.empty:\n",
    "        distances = eq_filtered.apply(lambda row: geodesic((row['latitude'], row['longitude']), tokyo_coords).km, axis=1)\n",
    "        min_dist_list.append(distances.min())\n",
    "        max_dist_list.append(distances.max())\n",
    "    else:\n",
    "        min_dist_list.append(np.nan)\n",
    "        max_dist_list.append(np.nan)\n",
    "\n",
    "# Create the combined df\n",
    "df = df_n225.copy()\n",
    "df['num'] = num_list\n",
    "df['sum'] = sum_list\n",
    "df['max'] = max_list\n",
    "df['avg'] = avg_list\n",
    "df['min_dist'] = min_dist_list\n",
    "df['max_dist'] = max_dist_list\n",
    "\n",
    "# Display result\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>change</th>\n",
       "      <th>num</th>\n",
       "      <th>sum</th>\n",
       "      <th>max</th>\n",
       "      <th>avg</th>\n",
       "      <th>min_dist</th>\n",
       "      <th>max_dist</th>\n",
       "      <th>rev_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>^N225</td>\n",
       "      <td>2008-08-01 00:00:00+09:00</td>\n",
       "      <td>-181.980469</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^N225</td>\n",
       "      <td>2008-08-04 00:00:00+09:00</td>\n",
       "      <td>-150.100586</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>^N225</td>\n",
       "      <td>2008-08-05 00:00:00+09:00</td>\n",
       "      <td>-42.349609</td>\n",
       "      <td>1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>4818.209478</td>\n",
       "      <td>4818.209478</td>\n",
       "      <td>0.000208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>^N225</td>\n",
       "      <td>2008-08-06 00:00:00+09:00</td>\n",
       "      <td>195.459961</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3159.157480</td>\n",
       "      <td>3159.157480</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>^N225</td>\n",
       "      <td>2008-08-07 00:00:00+09:00</td>\n",
       "      <td>-133.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker                      date      change  num  sum  max  avg  \\\n",
       "0  ^N225 2008-08-01 00:00:00+09:00 -181.980469    0  0.0  0.0  0.0   \n",
       "1  ^N225 2008-08-04 00:00:00+09:00 -150.100586    0  0.0  0.0  0.0   \n",
       "2  ^N225 2008-08-05 00:00:00+09:00  -42.349609    1  6.3  6.3  6.3   \n",
       "3  ^N225 2008-08-06 00:00:00+09:00  195.459961    1  6.0  6.0  6.0   \n",
       "4  ^N225 2008-08-07 00:00:00+09:00 -133.000000    0  0.0  0.0  0.0   \n",
       "\n",
       "      min_dist     max_dist  rev_dist  \n",
       "0     0.000000     0.000000  0.000000  \n",
       "1     0.000000     0.000000  0.000000  \n",
       "2  4818.209478  4818.209478  0.000208  \n",
       "3  3159.157480  3159.157480  0.000317  \n",
       "4     0.000000     0.000000  0.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Final preparations of the dataframe\n",
    "\n",
    "#Changing the missing values to 0. Have to see if not doing this is preferable\n",
    "df[['sum', 'max', 'avg', 'min_dist', 'max_dist']] = df[['sum', 'max', 'avg', 'min_dist', 'max_dist']].fillna(0)\n",
    "\n",
    "#In this model I used 1 divided by minimum distance of the earthquakes in each time period as one of the factors, assuming the further the earthquake, the lesser its effect\n",
    "df['rev_dist'] = df['min_dist'].apply(lambda x: 0 if x == 0 else 1 / x)\n",
    "#Creating a csv file to be able to looking at it by Excel, as it might be faster/easier to go through\n",
    "df.to_csv('data/test/testn225.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 change   R-squared:                       0.017\n",
      "Model:                            OLS   Adj. R-squared:                  0.008\n",
      "Method:                 Least Squares   F-statistic:                     1.960\n",
      "Date:                Tue, 04 Feb 2025   Prob (F-statistic):              0.120\n",
      "Time:                        15:34:57   Log-Likelihood:                -2276.7\n",
      "No. Observations:                 344   AIC:                             4561.\n",
      "Df Residuals:                     340   BIC:                             4577.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -26.9750     13.027     -2.071      0.039     -52.598      -1.351\n",
      "num           17.8908     18.084      0.989      0.323     -17.680      53.461\n",
      "max            2.1561      5.392      0.400      0.689      -8.449      12.762\n",
      "rev_dist    5653.0983   2.71e+04      0.209      0.835   -4.76e+04    5.89e+04\n",
      "==============================================================================\n",
      "Omnibus:                       62.623   Durbin-Watson:                   2.094\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              700.708\n",
      "Skew:                          -0.285   Prob(JB):                    6.97e-153\n",
      "Kurtosis:                       9.969   Cond. No.                     1.21e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.21e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#This part does the regreesion modeling\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define independent variables (num, max, rev_dist) and dependent variable (change)\n",
    "X = df[['num', 'max', 'rev_dist']]\n",
    "y = df['change']\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary of the regression results\n",
    "print(model.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
